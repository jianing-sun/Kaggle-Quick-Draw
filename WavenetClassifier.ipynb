{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import nbimporter\n",
    "from DataParser import generateDf, getXYfromDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetParams():\n",
    "    \"\"\" WaveNet parameter class, eaiser to be imported \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" n_filters: filter numbers for \n",
    "        \"\"\"\n",
    "        self.input_shape = (196, 3)\n",
    "        \n",
    "        self.output_shape = (340, )\n",
    "        \n",
    "        self.n_filters = 64 + 128\n",
    "        \n",
    "        self.kernel_size = 2\n",
    "        \n",
    "        self.dilation_depth = 8\n",
    "        \n",
    "        self.pool_size_1 = 4\n",
    "        \n",
    "        self.pool_size_2 = 8\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        \n",
    "        self.activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, i, wavenet_params):\n",
    "    tanh_out = layers.Conv1D(wavenet_params.n_filters, \n",
    "                      wavenet_params.kernel_size, \n",
    "                      dilation_rate = wavenet_params.kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_tanh' % (wavenet_params.kernel_size ** i), \n",
    "                      activation='tanh'\n",
    "                      )(x)\n",
    "    sigm_out = layers.Conv1D(wavenet_params.n_filters, \n",
    "                      wavenet_params.kernel_size, \n",
    "                      dilation_rate = wavenet_params.kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_sigm' % (wavenet_params.kernel_size ** i), \n",
    "                      activation='sigmoid'\n",
    "                      )(x)\n",
    "    z = layers.Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n",
    "    skip = layers.Conv1D(wavenet_params.n_filters, 1, name='skip_%d'%(i))(z)\n",
    "    res = layers.Add(name='residual_block_%d' % (i))([skip, x])\n",
    "    return res, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WaveNet(wavenet_params):\n",
    "    stroke_input = layers.Input(shape=wavenet_params.input_shape, name='featureInput')\n",
    "    \n",
    "    x = layers.Conv1D(wavenet_params.n_filters, wavenet_params.kernel_size, dilation_rate=1, \n",
    "                      padding='causal', name='dilated_conv_1')(stroke_input)\n",
    "    \n",
    "    skip_connections = []\n",
    "    \n",
    "    for i in range(1, wavenet_params.dilation_depth + 1):\n",
    "        x, skip = residual_block(x, i, wavenet_params)\n",
    "        skip_connections.append(skip)\n",
    "        \n",
    "    x = layers.Add(name='skip_connections')(skip_connections)\n",
    "#     x = layers.Activation('relu')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    x = layers.Conv1D(wavenet_params.n_filters, wavenet_params.pool_size_1, strides=1, \n",
    "                      padding='same', name='conv_5ms', activation='relu')(x)\n",
    "    \n",
    "    x = layers.Conv1D(wavenet_params.output_shape[0], wavenet_params.pool_size_2, padding='same', \n",
    "                      activation='relu', name='conv_500ms')(x)\n",
    "    \n",
    "    x = layers.Conv1D(wavenet_params.output_shape[0], wavenet_params.pool_size_2, padding='same', \n",
    "                      activation='relu', name='conv_500ms_target_shape')(x)\n",
    "    \n",
    "    x = layers.AveragePooling1D(wavenet_params.pool_size_2, padding='same',name = 'downsample_to_2Hz')(x)\n",
    "    \n",
    "    x = layers.Conv1D(wavenet_params.output_shape[0], \n",
    "                      (int)(wavenet_params.input_shape[0] / (wavenet_params.pool_size_1*wavenet_params.pool_size_2)), \n",
    "                      padding='same', name='final_conv')(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D(name='final_pooling')(x)\n",
    "    \n",
    "    x = layers.Activation(wavenet_params.activation, name='final_activation')(x)\n",
    "    \n",
    "    model = Model(input=stroke_input, output=x)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_accuracy(x,y): \n",
    "    return top_k_categorical_accuracy(x,y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_valid, y_valid):\n",
    "    date = datetime.datetime.today().strftime('%H_%M_%m_%d')\n",
    "    weight_save_path = './model/stroke_wn_%s' % date + '.h5'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(weight_save_path, monitor='val_loss',\n",
    "                                verbose=1, save_best_only=True, period=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, \n",
    "                                  patience=5, min_lr=1e-6, mode='auto')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "    callback = [checkpoint, early_stop, reduce_lr]\n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', top_3_accuracy])\n",
    "    model.fit(x_train, y_train, \n",
    "              validation_data=(x_valid, y_valid),\n",
    "              batch_size=wavenet_params.batch_size,\n",
    "              epochs=50,\n",
    "              callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(weight_path=None):\n",
    "    \"\"\" basic parameters \"\"\"\n",
    "    base_dir = '/Volumes/JS/QuickDraw'\n",
    "    test_path = os.path.join(base_dir, '/test_simplified.csv')\n",
    "    all_train_paths = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "    cols = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "    \n",
    "    wavenet_params = WaveNetParams()\n",
    "    \n",
    "    train_df, valid_df, test_df, word_encoder = generateDf(n_train=75, n_valid=7, n_test=5, \n",
    "                                                       n_strokes=196, path=all_train_paths)\n",
    "    x_train, y_train = getXYfromDf(train_df, word_encoder)\n",
    "    x_valid, y_valid = getXYfromDf(valid_df, word_encoder)\n",
    "    x_test, y_test = getXYfromDf(test_df, word_encoder)\n",
    "    \n",
    "    input_shape = x_train.shape[1:]\n",
    "    output_shape = y_train.shape[1:]\n",
    "    model = WaveNet(wavenet_params)\n",
    "    if weight_path is not None:\n",
    "        model.load_weights(weight_path)\n",
    "    train(model, x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"fe..., outputs=Tensor(\"fi...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "featureInput (InputLayer)       (None, 196, 3)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 196, 64)      448         featureInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 196, 64)      8256        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 196, 64)      8256        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 196, 64)      0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 196, 64)      4160        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 196, 64)      0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 196, 64)      8256        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 196, 64)      8256        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 196, 64)      0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 196, 64)      4160        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 196, 64)      0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 196, 64)      8256        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 196, 64)      8256        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 196, 64)      0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 196, 64)      4160        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 196, 64)      0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 196, 64)      8256        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 196, 64)      8256        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 196, 64)      0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 196, 64)      4160        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 196, 64)      0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 196, 64)      8256        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 196, 64)      8256        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 196, 64)      0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 196, 64)      4160        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 196, 64)      0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 196, 64)      8256        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 196, 64)      8256        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 196, 64)      0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 196, 64)      4160        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 196, 64)      0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 196, 64)      8256        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 196, 64)      8256        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 196, 64)      0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 196, 64)      4160        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 196, 64)      0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 196, 64)      8256        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 196, 64)      8256        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 196, 64)      0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 196, 64)      4160        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 196, 64)      0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 196, 64)      0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 196, 64)      16448       leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 196, 340)     174420      conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 196, 340)     925140      conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 25, 340)      0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 25, 340)      693940      downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (GlobalAveragePoo (None, 340)          0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "final_activation (Activation)   (None, 340)          0           final_pooling[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,975,772\n",
      "Trainable params: 1,975,772\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25500 samples, validate on 2380 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
